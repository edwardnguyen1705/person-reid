model:
  name: Baseline

  num_classes: 751
  feature_dim: 512

  backbone:
    name: resnet
    resnet:
      resnext: False
      with_se: False
      num_layer: 50
      with_d: False
      resnext_type: null
      with_ibn_a: False
      last_stride: 1
      activation:
        name: ReLU
        inplace: true

      attention: # build_attention for more information
        enable: False
        name: NonLocalBlock
        rd_ratio: -1 # -1 it mean hidden_channel = 1 in nonlocal block
        cam_batchnorm: False
        pam_batchnorm: False

      pretrained: True
      progress: True

    osnet:
      width: x1.0
      ibn: False

      attention:
        enable: False
        name: FPBAttention
        rd_ratio: -1 # it mean hidden_channel = 1 in nonlocal block
        cam_batchnorm: True
        pam_batchnorm: True

      activation:
        name: ReLU
        inplace: true
      pretrained: True

    osnet_ain:
      width: x1.0

      attention:
        enable: False
        name: FPBAttention
        rd_ratio: -1 # it mean hidden_channel = 1 in nonlocal block
        cam_batchnorm: True
        pam_batchnorm: True

      activation:
        name: ReLU
        inplace: true

      pretrained: True

    efficientnet:
      version: b0 # b1, b2, b3, b4
      remove_last_stride: False
      activation:
        name: SiLU
        inplace: true
      pretrained: True
      progress: True

    efficientnetv2_rw:
      version: efficientnetv2_rw_t # or, gc_efficientnetv2_rw_t, efficientnetv2_rw_s, efficientnetv2_rw_m
      activation:
        name: SiLU
        inplace: true
      pretrained: True
      progress: True

    mobilenetv3:
      activation1:
        name: ReLU
        inplace: true
      activation2:
        name: Hardswish
        inplace: true
      gate_fn:
        name: Hardsigmoid
        inplace: true
      pretrained: True
      progress: True

  pooling:
    name: AvgPooling2d

  neck:
    name: BNNeck
    out_channels: null # not work with BNNeck
    type_norm: 2d
    bias_freeze: True
    activation: # not work with BNNeck
      name: SiLU
      inplace: true

  head:
    name: Linear
    scale: null # not work with Linear
    margin: null # not work with Linear
    k: null # not work with Linear

  feature_pos: before
