timezone: Asia/Ho_Chi_Minh

wandb:
  project: rep-reid-v2
  entity: hiennguyen9874

data:
  name: market1501
  image_size: [256, 128]
  num_workers: 4
  train:
    batch_size: 128
    num_instances: 32 # num image per batch
    transform:
      autoaugmentation: False
      random_erasing: True
      cutout: False
      random2translate: False
      lgt: False
      random_grayscale: False

  test:
    batch_size: 256

model:
  name: Baseline

  num_classes: 751
  feature_dim: 512

  backbone:
    name: resnet
    resnet:
      resnext: False
      with_se: False
      num_layer: 50
      with_d: False
      resnext_type: null
      with_ibn_a: False
      last_stride: 1
      activation:
        name: ReLU
        inplace: true

      attention: # build_attention for more information
        enable: False
        name: NonLocalBlock
        rd_ratio: -1 # -1 it mean hidden_channel = 1 in nonlocal block
        cam_batchnorm: False
        pam_batchnorm: False

      pretrained: True
      progress: True

    osnet:
      width: x1.0
      ibn: False

      attention:
        enable: False
        name: FPBAttention
        rd_ratio: -1 # it mean hidden_channel = 1 in nonlocal block
        cam_batchnorm: True
        pam_batchnorm: True

      activation:
        name: ReLU
        inplace: true
      pretrained: True

    osnet_ain:
      width: x1.0

      attention:
        enable: False
        name: FPBAttention
        rd_ratio: -1 # it mean hidden_channel = 1 in nonlocal block
        cam_batchnorm: True
        pam_batchnorm: True

      activation:
        name: ReLU
        inplace: true

      pretrained: True

    efficientnet:
      version: b0 # b1, b2, b3, b4
      remove_last_stride: False
      activation:
        name: SiLU
        inplace: true
      pretrained: True
      progress: True

    efficientnetv2_rw:
      version: efficientnetv2_rw_t # or, gc_efficientnetv2_rw_t, efficientnetv2_rw_s, efficientnetv2_rw_m
      activation:
        name: SiLU
        inplace: true
      pretrained: True
      progress: True

    mobilenetv3:
      activation1:
        name: ReLU
        inplace: true
      activation2:
        name: Hardswish
        inplace: true
      gate_fn:
        name: Hardsigmoid
        inplace: true
      pretrained: True
      progress: True

    resnest:
      num_layer: 50
      last_stride: 2
      activation:
        name: ReLU
        inplace: True
      pretrained: True
      progress: True

  pooling:
    name: AvgPooling2d

  neck:
    name: BNNeck
    out_channels: null # not work with BNNeck
    type_norm: 2d
    bias_freeze: True
    activation: # not work with BNNeck
      name: SiLU
      inplace: true

  head:
    name: Linear
    scale: null # not work with Linear
    margin: null # not work with Linear
    k: null # not work with Linear

  feature_pos: before

loss:
  name: 1.0 * CrossEntropy + 1.0 * TripletLoss
  cross_entropy:
    epsilon: 0.1
    reduction: sum
  triplet:
    margin: 0.3
    distance_mode: euclidean
    hard_mining: True
    norm_feature: False
    eps: 1.0e-8
  cosface:
    margin: 0.3
    gamma: 128
  multi_similarity:
    alpha: 2.0
    beta: 50.0
    gamma: 1.0
    eps: 1.0e-8
  circle:
    margin: 0.3
    gamma: 128
  focal:
    gamma: 1.5
    alpha: 0.25
    reduction: sum

optimizer:
  name: adam
  lr: 0.00035
  specified_lr:
    enable: False
    lr: 0.1
    layers: [head]
  adam:
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0005
    amsgrad: False
  sgd:
    momentum: 0.9
    weight_decay: 0.0005
    dampening: 0
    nesterov: False
  adamW:
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0001

freeze:
  enable: False
  layers: [backbone]
  epochs: 10

lr_scheduler:
  enable: False
  name: MultiStepLR
  start: 1

  WarmupMultiStepLR:
    steps: [40, 90]
    gamma: 0.1
    warmup_factor: 0.01
    warmup_iters: 10
    warmup_method: linear

  ReduceLROnPlateau:
    factor: 0.1
    patience: 10
    min_lr: 1.0e-7

  MultiStepLR:
    steps: [40, 70]
    gamma: 0.1

  WarmupCosineAnnealingLR:
    max_iters: 120
    delay_iters: 30
    eta_min_lr: 7.0e-7
    warmup_factor: 0.01
    warmup_iters: 10
    warmup_method: linear

  CosineAnnealingLR:
    max_iters: 120
    eta_min_lr: 7.0e-7

  # OneCycleLR:
  #   epochs: 120
  #   max_lr: 0.01
  #   div_factor: 25
  #   anneal_strategy: cos # or linear

iters_to_accumulate: 1

ema:
  enable: False
  decay: 0.9998

clip_grad_norm_:
  enable: False
  max_norm: 10.0

trainer:
  epochs: 120
  amp: True

testing:
  distance_mode: euclidean
  flip_inference: False
  test_on_ema: False
